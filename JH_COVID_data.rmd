---
title: "Unified_COVID_Dataset"
author: "Jolene Branch"
date: "July 5, 2021"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Description: Importing and attempting to join multiple COVID-19 datasets.

```{r set working directory}
setwd("C:/School/Bellevue University/DSC 680 Applied Data Science/Project 1/Johns Hopkins COVID data")
getwd()
```

```{r}
install.packages("rio")  # new package, makes it easier to load different data types?
install.packages("DBI")  # for using SQL
install.packages("tidyr")  # so I can use spread()
install.packages("ggplot2")
install.packages("tidyverse")
install.packages("dplyr")
install.packages("readr")
library(DBI)
library(rio)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(readr)
devtools::install_github("tidyverse/tidyr")
```

# from https://github.com/CSSEGISandData/COVID-19_Unified-Dataset/blob/master/COVID-19.rds
# want to load it into R, then save as .csv so I can import it into SQL for easier joining.

```{r first rds file that I saved and am uploading into R}
JH_COVID_data <- readRDS("C:/School/Bellevue University/DSC 680 Applied Data Science/Project 1/Johns Hopkins COVID data/COVID-19.rds")
```


```{r}
head(JH_COVID_data)     # This is NOT tidy data.
```

```{r}
summary(JH_COVID_data)   
```

# oof.  This is massive.  It is because of the states/provinces, and over a year's worth of dates.  Let's start by getting rid of any row with a country code >2 characters long

```{r}
con <- dbConnect(RMySQL::MySQL(), dbname ="JH_COVID_data")
short <- dbGetQuery(con,"SELECT ID, Date, Cases, Cases_New, Type, Age, Sex, Source
                    FROM JH_COVID_data
                    WHERE CHAR_LENGTH(ID) <3")
summary(short)
```

```{r filter rows with length of country ID <3}
only_countries <- filter(JH_COVID_data, nchar(ID) < 3)
head(only_countries)
```

```{r get just most recent data}
last_date <- filter(only_countries, Date == '2021-06-11')
head(last_date)
```

# Looks like there are at least two different sources. Keep JHU only
```{r filter Source variable to only keep JHU}
JHU_cases_only <- filter(last_date, Source == 'JHU')
head(JHU_cases_only)
```

# ************I am stuck on this step.**********************
# Data needs to be 'spread' out.  Active/Confirmed/Deaths should be in separate columns.
# from p.154-155 in 'R for Data Science' by Hadley Wickham and Garrett Grolemund
# might have to do this twice - once for Cases and once for Cases_New, and store output in
# separate datasets.  Can always pull them back together in PowerBI
```{r measurement column spread}
spread_JHU_cases_only <- pivot_wider(JHU_cases_only,key = Type, value = Cases_New, Cases)
head(spread_JHU_cases_only)
```

```{r another try at long to wide}
data_reshape2 <- reshape(JHU_cases_only,
                         key = ID,
                         value = Type)
tail(data_reshape2)
```

# This is working better.  Leave it alone.
```{r second database from the same site that I saved and am uploading into R}
Static_COVID_data <- readRDS("C:/School/Bellevue University/DSC 680 Applied Data Science/Project 1/Johns Hopkins COVID data/COVID-19_Static.rds")
```

```{r}
head(Static_COVID_data)
```

# save as csv file, delete columns I won't be using, then re-import to R (or SSMS, or PowerBI)!
# Do NOT rerun this code!!!!!!!!!!!!!!!!!!!!
```{r send to csv file}
# write.csv(x=JH_COVID_data, file="JH_COVID_data.csv", row.names = FALSE)
# write.csv(x=Static_COVID_data, file="Static_COVID_data.csv", row.names = FALSE)
```

# tried unsuccessfully to merge the two files in SQLite, so not doing it in R, using code that
# looks a lot like SQL(!)
# from https://www.math.ucla.edu/~anderson/rw1001/library/base/html/merge.html
```{r reimport the two files and merge them}
geospatial_data <- read.csv('COVID-19_LUT_v2.csv',stringsAsFactors = FALSE)
clinical_data <- read.csv('Static_COVID_data_v2.csv', stringsAsFactors = FALSE)
merged_db <- merge(geospatial_data, clinical_data, by.x="ID", by.y="ID")
write.csv(x=merged_db, file ="merged_COVID_data.csv", row.names = FALSE)
head(merged_db)
```

```{r reimport the merged database}
my_data <- read.csv('merged_COVID_data.csv', stringsAsFactors = FALSE)
head(my_data)
```

# now go back and spread the JHU_Cases_Only from long to wide, then merge that dataframe with the my_data data frame.  Then pull THAT one into PowerBI.  I need the JHU_Cases_Only data or else I don't have any COVID data in my dataset!!!
# again, what that dataset looks like
```{r}
head(JHU_cases_only)
```

```{r}
summary(JHU_cases_only)
```

# It occurs to me that I do not need both 'Cases' and 'Cases_New.'  Let's get rid of some columns
```{r delete unecessary columns}
df1 = subset(JHU_cases_only, select = -c(Cases_New, Age, Sex, Date, Source))
head(df1)
```

```{r spread using pivot_wider()}
COVID_wide <- pivot_wider(df1, Type = key, Cases = value)
head(COVID_wide)
```

# from https://rdrr.io/cran/tidyr/src/R/spread.R
```{r}
library(dplyr)
wide_COVID <- spread(df1, key = Type, value = Cases)
head(wide_COVID)

```

# a classmate suggested that I check for duplicate rows, from https://stackoverflow.com/questions/47296552/using-spread-correctly-in-r-error
```{r}
df2 %>%
  distinct() %>%
  spread(Type, Cases)

head(df2)
```
# So that did not work, either.

# send df1 to csv and open it in another R file.  Only install one library; either dplyr or tidy
```{r send df1 to csv and reopen in another R script}
write.csv(x=df1, file="df1.csv", row.names = FALSE)
```

# pull back in the wide dataset
```{r}
manual_pivot_data <- read.csv('df4_manual_pivot_results.csv', stringsAsFactors = FALSE)
head(manual_pivot_data)
```

# merge manual_pivot_data with 
```{r}
merge_one <- merge(my_data, manual_pivot_data, by.x="ID", by.y="ID")
tail(merge_one)
```

# OK, so I FINALLY have all three datasets merged properly.  Now maybe delete some columns?  LIke the male/female ones?  But then do I need to create a derived chronic illness prevalence column (rather than the count that the dataset gives me)?
```{r delete more columns}
merge_one_trimmed = subset(merge_one, select = -c(Level, ISO1_3N))
head(merge_one_trimmed)
```

```{r send trimmed, merged db to .csv for import into PowerBI}
write.csv(x=merge_one_trimmed, file="merge_one_trimmed.csv", row.names = FALSE)
```

